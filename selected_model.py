# -*- coding: utf-8 -*-
"""Selected model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A3HP_STdeNuxRX6-vLdHaxgMXp1ROKVx

#IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np
import joblib
    
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import LabelEncoder

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, FunctionTransformer
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor

import matplotlib.pyplot as plt
import seaborn as sns

"""#CREATE A DATAFRAME"""

df_Outliers=pd.read_csv('Ordered data(Sheet1).csv')

"""##Remove outliers"""

# Calculate Q1 (25th percentile) and Q3 (75th percentile) for the 'YIELD' column
Q1 = df_Outliers['YIELD'].quantile(0.25)
Q3 = df_Outliers['YIELD'].quantile(0.75)

# Calculate IQR (Interquartile Range)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Filter out outliers
df = df_Outliers[(df_Outliers['YIELD'] >= lower_bound) & (df_Outliers['YIELD'] <= upper_bound)]

"""##Adding a trend column to replace the year"""

# Add a trend feature for the year
df['TREND'] = df['YEAR'] - df['YEAR'].min()

"""#DATA PREPROCESSING"""

# Drop the 'UNIT' column
df = df.drop(columns=['UNIT', 'YEAR'])

# One-Hot Encode 'REGION' and 'CROP'
one_hot_encoder = OneHotEncoder(sparse=False, drop=None)  # Avoid the dummy variable trap by dropping the first column
encoded_features = one_hot_encoder.fit_transform(df[['REGION', 'CROP']])

# Create a DataFrame for encoded features
encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(['REGION', 'CROP']))

# Combine the encoded features with the original dataset (excluding the original 'REGION' and 'CROP' columns)
data_encoded = pd.concat([df.drop(['REGION', 'CROP'], axis=1), encoded_df], axis=1)

# Reset the index for both DataFrames to ensure proper alignment
df_reset = df.reset_index(drop=True)
encoded_df_reset = encoded_df.reset_index(drop=True)

# Concatenate the encoded features with the original DataFrame (excluding 'REGION' and 'CROP')
data_encoded = pd.concat([df_reset.drop(['REGION', 'CROP'], axis=1), encoded_df_reset], axis=1)

# Check for NaN values after encoding
# print("NaN values after one-hot encoding and concatenation:")
# print(data_encoded.isna().sum())

"""#FEATURE SELECTION"""

# Feature engineering: create interaction features
data_encoded['HUMIDITY_TEMPERATURE'] = data_encoded['HUMIDITY'] * data_encoded['TEMPERATURE']

# Separate features and target
X = data_encoded.drop(columns=['YIELD'])
y = data_encoded['YIELD']

# Log transform the target variable if skewed
transformer = FunctionTransformer(np.log1p, validate=True)
y_transformed = transformer.fit_transform(y.values.reshape(-1, 1))

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""#MODEL DEVELOPMENT AND DEPLOYEMENT"""

# Define and optimize the Random Forest model
rf = RandomForestRegressor(random_state=42)
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search.fit(X_train_scaled, y_train)

# Get the best model
best_rf = grid_search.best_estimator_



# Evaluate the optimized Random Forest model
y_pred = best_rf.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
accuracy = np.mean(np.abs(y_test - y_pred) / y_test <= 0.1)

print(f"Optimized Random Forest Results:")
print(f"  Mean Squared Error (MSE): {mse:.4f}")
print(f"  RÂ² Score: {r2:.4f}")
print(f"  Accuracy (within 10% tolerance): {accuracy * 100:.2f}%\n")


# Save the model 
joblib.dump(best_rf, 'best_ml.pkl')
# Save the scaler
joblib.dump(scaler, 'scaler.pkl')
